{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chi/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.TextClassifier import *\n",
    "from ThematicTextClassify.Preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('Categorized_Links.csv')\n",
    "# add the title and description column together to form a text document\n",
    "df['Text'] = df['Title']+ df['Description']\n",
    "df = df.dropna(subset= ['Text'], axis = 0)\n",
    "\n",
    "# preprocess the newly defined Text column\n",
    "# preprocess_text is a function I build and saved in TextClassifier.py\n",
    "\n",
    "df['Processed Text'] = df['Text'].map(preprocess_text)\n",
    "\n",
    "df['processed_string'] =  [' '.join(text) for text in df['Processed Text']]\n",
    "\n",
    "df['Class'] = \"\"\n",
    "df['Class'] = df.apply(lambda df: 'Health Outcome' if (df['Category'] == 'Health Outcome') else df['Class'], axis =1)\n",
    "df['Class'] = df.apply(lambda df: 'Other' if (df['Category'] != 'Health Outcome') else df['Class'], axis =1)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values('Class')\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop_duplicates(['Link'],keep= 'first')\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, class_train, class_test = train_test_split(df,\n",
    "                                                    df['Class'],\n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Classifier Predictions \n",
    "* Multinomial Naive Bayes (CountVectorizer)\n",
    "* Logistic Regression (CountVectorizer)\n",
    "* Linear SVC (CountVectorizer)\n",
    "* mnb (tfidf)\n",
    "* Linear SVC(tfidf)\n",
    "* Logistic Regression (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to be classified \n",
    "full = pd.read_csv(\"NewData.csv\")\n",
    "\n",
    "# combine the title and description as text as our document\n",
    "full['Text'] = full['Title'] +full['Description']\n",
    "full['Processed Text'] = full['Text'].map(preprocess_text)\n",
    "full['processed_string'] =  [' '.join(text) for text in full['Processed Text']]\n",
    "full = full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chi/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# first model \n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.25, min_df =1, ngram_range = (1,3)),MultinomialNB(alpha=0.75), text_train, class_train, full )\n",
    "model1 = full[full['Class'] == 'Health Outcome']\n",
    "\n",
    "# second model \n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.25, min_df =3, ngram_range = (1,2), norm = 'l2'), MultinomialNB(alpha=0.25), text_train, class_train, full)\n",
    "model2 = full[full['Class'] == 'Health Outcome']\n",
    "\n",
    "# third model \n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,1)), LogisticRegression(C=0.25, penalty = 'l2'), text_train, class_train, full)\n",
    "model3 = full[full['Class'] == 'Health Outcome']\n",
    "\n",
    "# fourth model\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,1), norm = None),LogisticRegression(C=0.25, penalty = 'l1'), text_train, class_train, full)\n",
    "model4 = full[full['Class'] == 'Health Outcome']\n",
    "\n",
    "\n",
    "# fifth model\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.5, min_df =1, ngram_range = (1,1)),LinearSVC(C=0.05), text_train, class_train, full)\n",
    "model5 = full[full['Class'] == 'Health Outcome']\n",
    "\n",
    "# sixth model\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,1)),LinearSVC(C=0.25), text_train, class_train, full)\n",
    "model6 = full[full['Class'] == 'Health Outcome']\n",
    "\n",
    "# seventh model \n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.75, min_df =2, ngram_range = (1,2)),XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7), text_train, class_train, full)\n",
    "model7 = full[full['Class'] == 'Health Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of classified data set before dropping duplicates:  521\n",
      "Length of classified data set after dropping duplicates:  137\n"
     ]
    }
   ],
   "source": [
    "frames = [model1, model2, model3, model4, model5, model6, model7]\n",
    "\n",
    "result_frame = pd.concat(frames)\n",
    "print(\"Length of classified data set before dropping duplicates: \", len(result_frame))\n",
    "result_frame = result_frame.drop_duplicates(['Link'],keep= 'last')\n",
    "print(\"Length of classified data set after dropping duplicates: \", len(result_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HealthOutcomecsv = result_frame[['Title', 'Description', 'Link', 'Class']]\n",
    "HealthOutcomecsv.to_csv('HealthOutcome.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Training Data, Holdout, and New data to be Classified (CountVectorizer)\n",
    "* Note: It is not likely to have individual vectorizer for each model. Therefore we would use countvectorizer with the most popular tuning parameters given from the best models above (optmized using GridSearchCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "countvect = CountVectorizer(max_df= 0.5, min_df =2, ngram_range = (1,2))\n",
    "X_train = countvect.fit_transform(text_train['processed_string'])\n",
    "X_train = X_train.toarray()\n",
    "y_train = class_train.replace(to_replace = \"Health Outcome\", value = 1)\n",
    "y_train = y_train.replace(to_replace = \"Other\", value = 0)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = countvect.transform(text_test['processed_string'])\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "y_test = class_test.replace(to_replace = \"Health Outcome\", value = 1)\n",
    "y_test  = y_test.replace(to_replace = \"Other\", value = 0)\n",
    "y_test  = y_test.values\n",
    "\n",
    "X_full = countvect.transform(full['processed_string'])\n",
    "X_full = X_full.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (CountVectorizer)\n",
    "## Using Logistic Regression as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.05) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.85 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (Logisti Regression Meta Classifier)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Health Outcome       1.00      0.77      0.87        13\n",
      "         Other       0.94      1.00      0.97        48\n",
      "\n",
      "      accuracy                           0.95        61\n",
      "     macro avg       0.97      0.88      0.92        61\n",
      "  weighted avg       0.95      0.95      0.95        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.75)\n",
    "stack_clf2 = LogisticRegression(C=0.25, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 300, random_state = 2)\n",
    "stack_clf5 = XGBClassifier(max_depth = 6, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "\n",
    "meta_clf = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "sclf_log = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4,stack_clf5, sclf_log]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_log = sclf_log.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_log.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Health Outcome')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (Logisti Regression Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (CountVectorizer)\n",
    "## Using XGboost as the Meta Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.05) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.85 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Health Outcome       0.92      0.85      0.88        13\n",
      "         Other       0.96      0.98      0.97        48\n",
      "\n",
      "      accuracy                           0.95        61\n",
      "     macro avg       0.94      0.91      0.92        61\n",
      "  weighted avg       0.95      0.95      0.95        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.75)\n",
    "stack_clf2 = LogisticRegression(C=0.25, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 300, random_state = 2)\n",
    "stack_clf5 = XGBClassifier(max_depth = 6, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "meta_clf = XGBClassifier(max_depth = 6, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "sclf_XGB = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_XGB]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost','StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_XGB = sclf_XGB.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Health Outcome')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.05) [VotingClassifier]\n",
      "\n",
      "\n",
      "Voting Classifier (CountVect) Clasification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Health Outcome       1.00      0.69      0.82        13\n",
      "         Other       0.92      1.00      0.96        48\n",
      "\n",
      "      accuracy                           0.93        61\n",
      "     macro avg       0.96      0.85      0.89        61\n",
      "  weighted avg       0.94      0.93      0.93        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vote_clf1 = MultinomialNB(alpha=0.25)\n",
    "vote_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "vote_clf3 = LinearSVC(C=0.15)\n",
    "vote_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "vote_clf5 = XGBClassifier(max_depth = 6, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('Multinomial Naive Bayes', vote_clf1), ('Logistic Regression Classifier', vote_clf2), ('LinearSVC', vote_clf3), ('RandomForestClassifier', vote_clf4), ('XGBoost', vote_clf5)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train, y_train)\n",
    "\n",
    "classifiers = [vote_clf1, vote_clf2, vote_clf3, vote_clf4, vote_clf5, eclf1]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'VotingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "predicted = eclf1.predict(X_test)\n",
    "eclf1.score(X_test, y_test)\n",
    "\n",
    "pred_results = []\n",
    "for i in predicted:\n",
    "    if i == 1:\n",
    "        pred_results.append('Health Outcome')\n",
    "    else:\n",
    "        pred_results.append('Other')\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Voting Classifier (CountVect) Clasification Report\")\n",
    "print(classification_report(class_test.tolist(),pred_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Data (tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "tfidf = TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,2))\n",
    "X_train = tfidf.fit_transform(text_train['processed_string'])\n",
    "X_train = X_train.toarray()\n",
    "y_train = class_train.replace(to_replace = \"Health Outcome\", value = 1)\n",
    "y_train = y_train.replace(to_replace = \"Other\", value = 0)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = tfidf.transform(text_test['processed_string'])\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "y_test = class_test.replace(to_replace = \"Health Outcome\", value = 1)\n",
    "y_test  = y_test.replace(to_replace = \"Other\", value = 0)\n",
    "y_test  = y_test.values\n",
    "\n",
    "X_full = tfidf.transform(full['processed_string'])\n",
    "X_full = X_full.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (tfidfVectorizer)\n",
    "## Using Logistic Regression as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.83 (+/- 0.00) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.84 (+/- 0.01) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.02) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.94 (+/- 0.03) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (TF-IDF) Clasification Report (Logistic Regression as meta classifier)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Health Outcome       1.00      0.77      0.87        13\n",
      "         Other       0.94      1.00      0.97        48\n",
      "\n",
      "      accuracy                           0.95        61\n",
      "     macro avg       0.97      0.88      0.92        61\n",
      "  weighted avg       0.95      0.95      0.95        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=0.25, penalty = 'l1')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.25, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 0)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf = LogisticRegression(C=0.25, penalty = 'l1')\n",
    "sclf_tfidf = StackingClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4,stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost','StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting new data\n",
    "sclf_tfidf = sclf_tfidf.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_tfidf.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Health Outcome')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "\n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (TF-IDF) Clasification Report (Logistic Regression as meta classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (tfidfVectorizer)\n",
    "## Using XGboost n as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.83 (+/- 0.00) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.84 (+/- 0.01) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.02) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.04) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (TF-IDF) Clasification Report (XGBoost as Meta Classifier)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Health Outcome       1.00      0.85      0.92        13\n",
      "         Other       0.96      1.00      0.98        48\n",
      "\n",
      "      accuracy                           0.97        61\n",
      "     macro avg       0.98      0.92      0.95        61\n",
      "  weighted avg       0.97      0.97      0.97        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=0.25, penalty = 'l1')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.25, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 0)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf =  XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "sclf_XGB_tfidf  = StackingClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4,stack_clf5, sclf_XGB_tfidf ]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost','StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data\n",
    "sclf_XGB_tfidf  = sclf_XGB_tfidf.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB_tfidf.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Health Outcome')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "\n",
    "# Print Classification report    \n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (TF-IDF) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier (tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.83 (+/- 0.00) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.84 (+/- 0.01) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.02) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.02) [VotingClassifier]\n",
      "\n",
      "\n",
      "Voting Classifier (TF-IDF) Clasification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Health Outcome       1.00      0.15      0.27        13\n",
      "         Other       0.81      1.00      0.90        48\n",
      "\n",
      "      accuracy                           0.82        61\n",
      "     macro avg       0.91      0.58      0.58        61\n",
      "  weighted avg       0.85      0.82      0.76        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vote_clf1 = MultinomialNB(alpha=0.25)\n",
    "vote_clf2 = LogisticRegression(C=0.25, penalty = 'l1')\n",
    "vote_clf3 = SVC(kernel='linear', C= 0.25, probability=True)\n",
    "vote_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 0)\n",
    "vote_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "eclf1_tfidf = VotingClassifier(estimators=[('Multinomial Naive Bayes', vote_clf1), ('Logistic Regression Classifier', vote_clf2), ('LinearSVC', vote_clf3), ('RandomForestClassifier', vote_clf4), ('XGBoost', vote_clf5)], voting='hard')\n",
    "eclf1_tfidf = eclf1_tfidf.fit(X_train, y_train)\n",
    "\n",
    "classifiers = [vote_clf1, vote_clf2, vote_clf3, vote_clf4, vote_clf5, eclf1_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost', 'VotingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "    \n",
    "# Predicting new data \n",
    "predicted = eclf1_tfidf.predict(X_test)\n",
    "eclf1_tfidf.score(X_test, y_test)\n",
    "\n",
    "pred_results = []\n",
    "for i in predicted:\n",
    "    if i == 1:\n",
    "        pred_results.append('Health Outcome')\n",
    "    else:\n",
    "        pred_results.append('Other')\n",
    "\n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Voting Classifier (TF-IDF) Clasification Report\")\n",
    "print(classification_report(class_test.tolist(),pred_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending the New Categories on to the Data set\n",
    "* We pick the one model that has the highest precision/f1-score to be our model \n",
    "* Best model is: `Stacked Classifier (tfidfVectorizer) Using XGboost as meta_classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Other             1609\n",
       "Health Outcome      88\n",
       "Name: BestModelClassification, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full['BestModelClassification'] = sclf_XGB_tfidf.predict(X_full)\n",
    "full['BestModelClassification'] = full['BestModelClassification'] .replace(to_replace = 1, value = \"Health Outcome\")\n",
    "full['BestModelClassification'] = full['BestModelClassification'] .replace(to_replace = 0, value = \"Other\")\n",
    "print(len(full[full['BestModelClassification'] == 'Health Outcome']))\n",
    "full['BestModelClassification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full[full['BestModelClassification'] == 'Health Outcome']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"BestHealthOutcome.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
