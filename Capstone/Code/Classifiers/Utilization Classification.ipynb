{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chi/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.TextClassifier import *\n",
    "from ThematicTextClassify.Preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('Categorized_Links.csv')\n",
    "# add the title and description column together to form a text document\n",
    "df['Text'] = df['Title']+ df['Description']\n",
    "df = df.dropna(subset= ['Text'], axis = 0)\n",
    "\n",
    "# preprocess the newly defined Text column\n",
    "# preprocess_text is a function I build and saved in TextClassifier.py\n",
    "\n",
    "df['Processed Text'] = df['Text'].map(preprocess_text)\n",
    "\n",
    "df['processed_string'] =  [' '.join(text) for text in df['Processed Text']]\n",
    "\n",
    "df['Class'] = \"\"\n",
    "df['Class'] = df.apply(lambda df: 'Utilization' if (df['Category'] == 'Utilization') else df['Class'], axis =1)\n",
    "df['Class'] = df.apply(lambda df: 'Other' if (df['Category'] != 'Utilization') else df['Class'], axis =1)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values('Class',ascending = False)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop_duplicates(['Link'],keep= 'first')\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, class_train, class_test = train_test_split(df,\n",
    "                                                    df['Class'],\n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Classifier Predictions \n",
    "### We have already tuned the classifier with optmized parameters, we will then combine classification results on the new data and drop the duplicates\n",
    "* Multinomial Naive Bayes (CountVectorizer/tfidf)\n",
    "* Logistic Regression (CountVectorizer/tfidf)\n",
    "* Linear SVC (CountVectorizer/tfidf)\n",
    "* RandomForestClassifier(CountVectorizer/tfidf)\n",
    "* XGBoost(CountVectorizer/tfidf)\n",
    "##### Note: This method is not necessarily the best way since it introduces a lot of error in our classification results, but we are lenient since we want to include as much information as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to be classified \n",
    "full = pd.read_csv(\"NewData.csv\")\n",
    "\n",
    "# combine the title and description as text as our document\n",
    "full['Text'] = full['Title'] +full['Description']\n",
    "full['Processed Text'] = full['Text'].map(preprocess_text)\n",
    "full['processed_string'] =  [' '.join(text) for text in full['Processed Text']]\n",
    "full = full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chi/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# first model \n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.25, min_df =1, ngram_range = (1,2)),MultinomialNB(alpha=0.25), text_train, class_train, full )\n",
    "model1 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "# second model \n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.25, min_df =3, ngram_range = (1,1)),MultinomialNB(alpha=0.5), text_train, class_train, full)\n",
    "model2 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "\n",
    "# third model \n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.75, min_df =3, ngram_range = (1,1)), LogisticRegression(C=0.5, penalty = 'l2'), text_train, class_train, full)\n",
    "model3 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "\n",
    "# fourth model\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.25, min_df =3, ngram_range = (1,2)), LogisticRegression(C=1.0, penalty = 'l1'), text_train, class_train, full)\n",
    "model4 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "# fifth model\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.75, min_df =3, ngram_range = (1,2)),LinearSVC(C=0.05), text_train, class_train, full)\n",
    "model5 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "\n",
    "# sixth model\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.25, min_df =2, ngram_range = (1,1)),LinearSVC(C=0.25), text_train, class_train, full)\n",
    "model6 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "# seventh model\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.25, min_df =3, ngram_range = (1,1)),RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 1), text_train, class_train, full)\n",
    "model7 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "# eigth model \n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.75, min_df =3, ngram_range = (1,2)),RandomForestClassifier(max_depth =4, n_estimators = 300, random_state = 1), text_train, class_train, full)\n",
    "model8 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "# ninth model\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.25, min_df =2, ngram_range = (1,1)),XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.5), text_train, class_train, full)\n",
    "model9 = full[full['Class'] == 'Utilization']\n",
    "\n",
    "# tenth model \n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.75, min_df =1, ngram_range = (1,2)),XGBClassifier(max_depth = 5, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7), text_train, class_train, full)\n",
    "model10 = full[full['Class'] == 'Utilization']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of classified data set before dropping duplicates:  301\n",
      "Length of classified data set after dropping duplicates:  86\n"
     ]
    }
   ],
   "source": [
    "# Concat data sets \n",
    "frames = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10]\n",
    "\n",
    "result_frame = pd.concat(frames)\n",
    "print(\"Length of classified data set before dropping duplicates: \", len(result_frame))\n",
    "result_frame = result_frame.drop_duplicates(['Link'],keep= 'last')\n",
    "print(\"Length of classified data set after dropping duplicates: \", len(result_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilizationcsv = result_frame[['Title', 'Description', 'Link','Class']]\n",
    "utilizationcsv = utilizationcsv.reset_index(drop=True)\n",
    "utilizationcsv.to_csv('utilization.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Link</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Combining nutrient intake from food/beverages ...</td>\n",
       "      <td>This article describes methods for combining ...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/82-003-x/2...</td>\n",
       "      <td>Utilization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Impact of identifying plausible respondents on...</td>\n",
       "      <td>A 24-hour dietary recall from 16,190 responde...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/82-003-x/2...</td>\n",
       "      <td>Utilization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Impact of number of repeat 24 hour recall inte...</td>\n",
       "      <td>National Food and Nutrition Surveys provide c...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/11-522-x/2...</td>\n",
       "      <td>Utilization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Nutrient intakes from food, 2015 Archived</td>\n",
       "      <td>This is a health fact sheet about the nutrien...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/82-625-x/2...</td>\n",
       "      <td>Utilization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Protein sources in the Canadian diet, 2015</td>\n",
       "      <td>This infographic presents results from the 20...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/11-627-m/1...</td>\n",
       "      <td>Utilization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "81  Combining nutrient intake from food/beverages ...   \n",
       "82  Impact of identifying plausible respondents on...   \n",
       "83  Impact of number of repeat 24 hour recall inte...   \n",
       "84          Nutrient intakes from food, 2015 Archived   \n",
       "85         Protein sources in the Canadian diet, 2015   \n",
       "\n",
       "                                          Description  \\\n",
       "81   This article describes methods for combining ...   \n",
       "82   A 24-hour dietary recall from 16,190 responde...   \n",
       "83   National Food and Nutrition Surveys provide c...   \n",
       "84   This is a health fact sheet about the nutrien...   \n",
       "85   This infographic presents results from the 20...   \n",
       "\n",
       "                                                 Link        Class  \n",
       "81  https://www150.statcan.gc.ca/n1/pub/82-003-x/2...  Utilization  \n",
       "82  https://www150.statcan.gc.ca/n1/pub/82-003-x/2...  Utilization  \n",
       "83  https://www150.statcan.gc.ca/n1/pub/11-522-x/2...  Utilization  \n",
       "84  https://www150.statcan.gc.ca/n1/pub/82-625-x/2...  Utilization  \n",
       "85  https://www150.statcan.gc.ca/n1/pub/11-627-m/1...  Utilization  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utilizationcsv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Training Data, Holdout, and New data to be Classified (CountVectorizer)\n",
    "* Note: It is not likely to have individual vectorizer for each model. Therefore we would use countvectorizer with the most popular tuning parameters given from the best models above (optmized using GridSearchCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "countvect = CountVectorizer(max_df= 0.5, min_df =2, ngram_range = (1,2))\n",
    "X_train = countvect.fit_transform(text_train['processed_string'])\n",
    "X_train = X_train.toarray()\n",
    "y_train = class_train.replace(to_replace = \"Utilization\", value = 1)\n",
    "y_train = y_train.replace(to_replace = \"Other\", value = 0)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = countvect.transform(text_test['processed_string'])\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "y_test = class_test.replace(to_replace = \"Utilization\", value = 1)\n",
    "y_test  = y_test.replace(to_replace = \"Other\", value = 0)\n",
    "y_test  = y_test.values\n",
    "\n",
    "X_full = countvect.transform(full['processed_string'])\n",
    "X_full = X_full.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (CountVectorizer)\n",
    "## Using Logistic Regression as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.91 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (Logistic Regression Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.89      0.97      0.93        34\n",
      " Utilization       0.89      0.67      0.76        12\n",
      "\n",
      "    accuracy                           0.89        46\n",
      "   macro avg       0.89      0.82      0.85        46\n",
      "weighted avg       0.89      0.89      0.89        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=0.5, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 1)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.5)\n",
    "\n",
    "meta_clf = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "sclf_log = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_log]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_log = sclf_log.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_log.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Utilization')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "    \n",
    "# Print Classification report   \n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (Logistic Regression Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (CountVectorizer)\n",
    "## Using XGBoost as the Meta Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.91 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.92      0.97      0.94        34\n",
      " Utilization       0.90      0.75      0.82        12\n",
      "\n",
      "    accuracy                           0.91        46\n",
      "   macro avg       0.91      0.86      0.88        46\n",
      "weighted avg       0.91      0.91      0.91        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=0.5, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 1)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.5)\n",
    "\n",
    "meta_clf = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.5)\n",
    "\n",
    "sclf_XGB = StackingClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4,stack_clf5, sclf_XGB]\n",
    "\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_XGB = sclf_XGB.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Utilization')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.92 (+/- 0.04) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.91 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.02) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.89      0.94      0.91        34\n",
      " Utilization       0.80      0.67      0.73        12\n",
      "\n",
      "    accuracy                           0.87        46\n",
      "   macro avg       0.84      0.80      0.82        46\n",
      "weighted avg       0.87      0.87      0.87        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=0.5, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 1)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.5)\n",
    "\n",
    "meta_clf = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.5)\n",
    "\n",
    "sclf_XGB = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4,stack_clf5, sclf_XGB]\n",
    "\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_XGB = sclf_XGB.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Utilization')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Data (tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "tfidf = TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,2))\n",
    "X_train = tfidf.fit_transform(text_train['processed_string'])\n",
    "X_train = X_train.toarray()\n",
    "y_train = class_train.replace(to_replace = \"Utilization\", value = 1)\n",
    "y_train = y_train.replace(to_replace = \"Other\", value = 0)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = tfidf.transform(text_test['processed_string'])\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "y_test = class_test.replace(to_replace = \"Utilization\", value = 1)\n",
    "y_test  = y_test.replace(to_replace = \"Other\", value = 0)\n",
    "y_test  = y_test.values\n",
    "\n",
    "X_full = tfidf.transform(full['processed_string'])\n",
    "X_full = X_full.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (tfidfVectorizer)\n",
    "## Using Logistic Regression as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.02) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.01) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.01) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.90 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.91 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (TF-IDF) Clasification Report (Logistic Regression as meta classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.89      1.00      0.94        34\n",
      " Utilization       1.00      0.67      0.80        12\n",
      "\n",
      "    accuracy                           0.91        46\n",
      "   macro avg       0.95      0.83      0.87        46\n",
      "weighted avg       0.92      0.91      0.91        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.5)\n",
    "stack_clf2 = LogisticRegression(C=1.0, penalty = 'l1')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.25, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 300, random_state = 1)\n",
    "stack_clf5 = XGBClassifier(max_depth = 5, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf = LogisticRegression(C=0.5, penalty = 'l2')\n",
    "sclf_tfidf = StackingClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost', 'StackingClassifier']\n",
    "\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_tfidf = sclf_tfidf.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_tfidf.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Utilization')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (TF-IDF) Clasification Report (Logistic Regression as meta classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (tfidfVectorizer)\n",
    "## Using XGBoost n as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.02) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.01) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.01) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.90 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.91 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.03) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (TF-IDF) Clasification Report (XGBoost as Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.92      0.97      0.94        34\n",
      " Utilization       0.90      0.75      0.82        12\n",
      "\n",
      "    accuracy                           0.91        46\n",
      "   macro avg       0.91      0.86      0.88        46\n",
      "weighted avg       0.91      0.91      0.91        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.5)\n",
    "stack_clf2 = LogisticRegression(C=1.0, penalty = 'l1')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.25, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 300, random_state = 1)\n",
    "stack_clf5 = XGBClassifier(max_depth = 5, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf = XGBClassifier(max_depth = 5, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "sclf_XGB_tfidf = StackingClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_XGB_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_XGB_tfidf = sclf_XGB_tfidf.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB_tfidf.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Utilization')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "        \n",
    "# Print Classification report    \n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (TF-IDF) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier (tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.95 (+/- 0.02) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.01) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.03) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.90 (+/- 0.01) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.91 (+/- 0.05) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.93 (+/- 0.04) [VotingClassifier]\n",
      "\n",
      "\n",
      "Voting Classifier (TF-IDF) Clasification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.85      1.00      0.92        34\n",
      " Utilization       1.00      0.50      0.67        12\n",
      "\n",
      "    accuracy                           0.87        46\n",
      "   macro avg       0.93      0.75      0.79        46\n",
      "weighted avg       0.89      0.87      0.85        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vote_clf1 = MultinomialNB(alpha=0.5)\n",
    "vote_clf2 = LogisticRegression(C=1.0, penalty = 'l1')\n",
    "vote_clf3 = LinearSVC(C=0.25, max_iter = 3000)\n",
    "vote_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 300, random_state = 1)\n",
    "vote_clf5 = XGBClassifier(max_depth = 5, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "eclf1_tfidf = VotingClassifier(estimators=[('Multinomial Naive Bayes', vote_clf1), ('Logistic Regression Classifier', vote_clf2), ('LinearSVC', vote_clf3), ('RandomForestClassifier', vote_clf4), ('XGBoost',vote_clf5)], voting='hard')\n",
    "eclf1_tfidf = eclf1_tfidf.fit(X_train, y_train)\n",
    "\n",
    "classifiers = [vote_clf1, vote_clf2, vote_clf3, vote_clf4, vote_clf5, eclf1_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'VotingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "    \n",
    "# Predicting new data \n",
    "predicted = eclf1_tfidf.predict(X_test)\n",
    "eclf1_tfidf.score(X_test, y_test)\n",
    "\n",
    "pred_results = []\n",
    "for i in predicted:\n",
    "    if i == 1:\n",
    "        pred_results.append('Utilization')\n",
    "    else:\n",
    "        pred_results.append('Other')\n",
    "\n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Voting Classifier (TF-IDF) Clasification Report\")\n",
    "print(classification_report(class_test.tolist(),pred_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending the New Categories on to the Data set\n",
    "* We pick the one model that has the highest precision/f1-score to be our model \n",
    "* Best model: `Stacked Classifier (Countvect) XGBoost as Meta Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Other          1638\n",
       "Utilization      59\n",
       "Name: BestModelClassification, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full['BestModelClassification'] = sclf_XGB.predict(X_full)\n",
    "full['BestModelClassification'] = full['BestModelClassification'] .replace(to_replace = 1, value = \"Utilization\")\n",
    "full['BestModelClassification'] = full['BestModelClassification'] .replace(to_replace = 0, value = \"Other\")\n",
    "print(len(full[full['BestModelClassification'] == 'Utilization']))\n",
    "full['BestModelClassification'] .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full[full['BestModelClassification'] == 'Utilization']\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(\"BestUtilization.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
