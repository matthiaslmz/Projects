{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the .py file which stores all our functions and library imports \n",
    "from ThematicTextClassify.TextClassifier import *\n",
    "from ThematicTextClassify.Preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumption Classification\n",
    "### This notebook will use the results of the best text classifiers tuned with GridsearchCV from the Classifiers folder to produce classification results\n",
    "* First method used would be to combine the classification results of multple tuned classifiers by merging the classification results on new data and dropping its duplicates\n",
    "* Second approach would be using two types of ensemble methods: Stacking and Voting to produce one classifer that classifies our new data \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('Categorized_Links.csv')\n",
    "# read the training data \n",
    "# Add title and description column together to form a text column as our document\n",
    "df['Text'] = df['Title']+ df['Description']\n",
    "df = df.dropna(subset= ['Text'], axis = 0)\n",
    "\n",
    "# preprocess the newly defined Text column\n",
    "# preprocess_text is a function written and saved in TextClassifier.py\n",
    "\n",
    "df['Processed Text'] = df['Text'].map(preprocess_text)\n",
    "\n",
    "# turn list to string format\n",
    "df['processed_string'] =  [' '.join(text) for text in df['Processed Text']]\n",
    "\n",
    "df['Class'] = \"\"\n",
    "df['Class'] = df.apply(lambda df: 'Consumption' if (df['Category'] == 'Consumption') else df['Class'], axis =1)\n",
    "df['Class'] = df.apply(lambda df: 'Other' if (df['Category'] != 'Consumption') else df['Class'], axis =1)\n",
    "\n",
    "# drop duplicates we want consumption all at the top of the data set\n",
    "# therefore we can drop duplicates by the keeping the first one we see \n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values('Class')\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop_duplicates(['Link'],keep= 'first')\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set using train_test_split from sklearn \n",
    "text_train, text_test, class_train, class_test = train_test_split(df,\n",
    "                                                    df['Class'],\n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=509)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Classifier Predictions \n",
    "### We have already tuned the classifier with optmized parameters, we will then combine classification results on the new data and drop the duplicates\n",
    "* Multinomial Naive Bayes (CountVectorizer/tfidf)\n",
    "* Logistic Regression (CountVectorizer/tfidf)\n",
    "* Linear SVC (CountVectorizer/tfidf)\n",
    "* RandomForestClassifier(CountVectorizer/tfidf)\n",
    "* XGBoost (CountVectorizer/tfidf)\n",
    "\n",
    "### This method is not such a great way since it introduces a lot of error in our classification results, but lets just test it out and see\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data to be classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to be classified \n",
    "full = pd.read_csv(\"NewData.csv\")\n",
    "\n",
    "# combine the title and description as text as our document\n",
    "full['Text'] = full['Title'] +full['Description']\n",
    "full['Processed Text'] = full['Text'].map(preprocess_text)\n",
    "full['processed_string'] =  [' '.join(text) for text in full['Processed Text']]\n",
    "full = full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify New Data with the 5 Classifiers with optimized tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MultinomialNB (CountVect)\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.25, min_df =1, ngram_range = (2,3)),MultinomialNB(alpha=0.25), text_train, class_train, full )\n",
    "model1 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# MultinomialNB (TFIDF)\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.25, min_df =1, ngram_range = (2,3)),MultinomialNB(alpha=0.5), text_train, class_train, full)\n",
    "model2 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "#  LogisticRegression (CountVect)\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.5, min_df =1, ngram_range = (1,3)), LogisticRegression(C=1.0, penalty = 'l2'), text_train, class_train, full)\n",
    "model3 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# LogisticRegression (TFIDF)\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,3), norm = None),LogisticRegression(C=1.0, penalty = 'l2'), text_train, class_train, full)\n",
    "model4 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# Linear SVC (CountVect)\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.5, min_df =2, ngram_range = (1,1)),LinearSVC(C=0.15), text_train, class_train, full)\n",
    "model5 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# Linear SVC (TFIDF)\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.5, min_df = 2, ngram_range = (1,3), norm = None),LinearSVC(C=0.05, max_iter = 3000), text_train, class_train, full)\n",
    "model6 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# Random Forest (CountVect)\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,1)),RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3), text_train, class_train, full)\n",
    "model7 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# Random Forest (TFIDF)\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,1), norm = None),RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3), text_train, class_train, full)\n",
    "model8 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# XGBoost (CountVect)\n",
    "full['Class'] = feature_pipe(CountVectorizer(max_df= 0.75, min_df =3, ngram_range = (1,2)),XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7), text_train, class_train, full)\n",
    "model9 = full[full['Class'] == 'Consumption']\n",
    "\n",
    "# XGBoost (TFIDF)\n",
    "full['Class'] = feature_pipe(TfidfVectorizer(max_df= 0.5, min_df =1, ngram_range = (1,2)),XGBClassifier(max_depth = 6, seed = 2, random_state=1995,colsample_bytree=0.3, subsample=0.7), text_train, class_train, full)\n",
    "model10 = full[full['Class'] == 'Consumption']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of classified data set before dropping duplicates:  1198\n",
      "Length of classified data set after dropping duplicates:  300\n"
     ]
    }
   ],
   "source": [
    "# Concat data sets \n",
    "frames = [model1, model2, model3, model4, model5, model6, model7, model8]\n",
    "\n",
    "result_frame = pd.concat(frames)\n",
    "print(\"Length of classified data set before dropping duplicates: \", len(result_frame))\n",
    "result_frame = result_frame.drop_duplicates(['Link'],keep= 'last')\n",
    "print(\"Length of classified data set after dropping duplicates: \", len(result_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "consumptioncsv = result_frame[['Title', 'Description', 'Link','Class']]\n",
    "consumptioncsv = consumptioncsv.reset_index(drop=True)\n",
    "consumptioncsv.to_csv('Consumption.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets take a look at the data set that has been newly classified\n",
    "* It seems somewhat ok, but lets try out the ensemble methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Link</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Nutrient intakes from food, 2015 Archived</td>\n",
       "      <td>This is a health fact sheet about the nutrien...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/82-625-x/2...</td>\n",
       "      <td>Consumption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Sodium consumption at all ages Archived</td>\n",
       "      <td>This article examines the amount of sodium th...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/82-003-x/2...</td>\n",
       "      <td>Consumption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Studying scenarios of nutrition intervention: ...</td>\n",
       "      <td>Using data from the Canadian Community Health...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/11-522-x/2...</td>\n",
       "      <td>Consumption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>The eating habits of Canadians Archived</td>\n",
       "      <td>Over the past 25 years, the eating habits and...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/61f0019x/6...</td>\n",
       "      <td>Consumption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Trends and correlates of frequency of fruit an...</td>\n",
       "      <td>Based on annual data from the Canadian Commun...</td>\n",
       "      <td>https://www150.statcan.gc.ca/n1/pub/82-003-x/2...</td>\n",
       "      <td>Consumption</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "295          Nutrient intakes from food, 2015 Archived   \n",
       "296            Sodium consumption at all ages Archived   \n",
       "297  Studying scenarios of nutrition intervention: ...   \n",
       "298            The eating habits of Canadians Archived   \n",
       "299  Trends and correlates of frequency of fruit an...   \n",
       "\n",
       "                                           Description  \\\n",
       "295   This is a health fact sheet about the nutrien...   \n",
       "296   This article examines the amount of sodium th...   \n",
       "297   Using data from the Canadian Community Health...   \n",
       "298   Over the past 25 years, the eating habits and...   \n",
       "299   Based on annual data from the Canadian Commun...   \n",
       "\n",
       "                                                  Link        Class  \n",
       "295  https://www150.statcan.gc.ca/n1/pub/82-625-x/2...  Consumption  \n",
       "296  https://www150.statcan.gc.ca/n1/pub/82-003-x/2...  Consumption  \n",
       "297  https://www150.statcan.gc.ca/n1/pub/11-522-x/2...  Consumption  \n",
       "298  https://www150.statcan.gc.ca/n1/pub/61f0019x/6...  Consumption  \n",
       "299  https://www150.statcan.gc.ca/n1/pub/82-003-x/2...  Consumption  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumptioncsv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble: Stacking Classifiers and Voting Classifiers\n",
    "* Using mlxtend: There are two ways of ensemble classifiers that we could use `StackingClassifier` and `StackingCVClassifier`. However, `StackingClassifier` in the standard stacking procedure, the first-level classifiers are fit to the same training set that is used prepare the inputs for the second-level classifier, which may lead to overfitting.\n",
    "\n",
    "* The more advanced way is to use `StackingCVClassifier`, which as explained in the mlxtend documentation uses the concept of cross-validation: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level classifier; in each round, the first-level classifiers are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level classifier. After the training of the `StackingCVClassifier`, the first-level classifiers are fit to the entire dataset as illustrated in the figure below.`\n",
    "\n",
    "* Then there is the `VotingClassifier`, which picks the predictions based on the majority votes of the base classifiers (weak learners). Performance is usually not as great as a stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingCVClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Training Data, Holdout, and New data to be Classified (CountVectorizer)\n",
    "* Note: It is not likely to have an individual vectorizer for each model. Therefore we would use the countvectorizer with the most popular tuning parameters used by the best models above (optmized using GridSearchCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "countvect = CountVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,2))\n",
    "X_train = countvect.fit_transform(text_train['processed_string'])\n",
    "X_train = X_train.toarray()\n",
    "y_train = class_train.replace(to_replace = \"Consumption\", value = 1)\n",
    "y_train = y_train.replace(to_replace = \"Other\", value = 0)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = countvect.transform(text_test['processed_string'])\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "y_test = class_test.replace(to_replace = \"Consumption\", value = 1)\n",
    "y_test  = y_test.replace(to_replace = \"Other\", value = 0)\n",
    "y_test  = y_test.values\n",
    "\n",
    "X_full = countvect.transform(full['processed_string'])\n",
    "X_full = X_full.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (CountVectorizer)\n",
    "## Using Logistic Regression as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.05) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.06) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.02) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.80 (+/- 0.05) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.04) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.06) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (Logistic Regression Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Consumption       0.95      0.95      0.95        22\n",
      "       Other       0.97      0.97      0.97        39\n",
      "\n",
      "    accuracy                           0.97        61\n",
      "   macro avg       0.96      0.96      0.96        61\n",
      "weighted avg       0.97      0.97      0.97        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "random.seed(1698)\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.15, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "sclf_log = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_log]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_log = sclf_log.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_log.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Consumption')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "    \n",
    "# Print Classification report   \n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (Logistic Regression Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (CountVectorizer)\n",
    "## Using XGBoost as the Meta Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.05) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.06) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.02) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.80 (+/- 0.05) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.04) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.04) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Consumption       0.91      0.95      0.93        22\n",
      "       Other       0.97      0.95      0.96        39\n",
      "\n",
      "    accuracy                           0.95        61\n",
      "   macro avg       0.94      0.95      0.95        61\n",
      "weighted avg       0.95      0.95      0.95        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.25)\n",
    "stack_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.15, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "stack_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "sclf_XGB = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4,stack_clf5, sclf_XGB]\n",
    "\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_XGB = sclf_XGB.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Consumption')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (CountVect) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.05) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.06) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.90 (+/- 0.03) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.80 (+/- 0.05) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.87 (+/- 0.04) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.90 (+/- 0.06) [VotingClassifier]\n",
      "\n",
      "\n",
      "Voting Classifier (CountVect) Clasification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Consumption       0.95      0.91      0.93        22\n",
      "       Other       0.95      0.97      0.96        39\n",
      "\n",
      "    accuracy                           0.95        61\n",
      "   macro avg       0.95      0.94      0.95        61\n",
      "weighted avg       0.95      0.95      0.95        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vote_clf1 = MultinomialNB(alpha=0.25)\n",
    "vote_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "vote_clf3 = LinearSVC(C=0.15)\n",
    "vote_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "vote_clf5 = XGBClassifier(max_depth = 4, seed = 1, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('Multinomial Naive Bayes', vote_clf1), ('Logistic Regression Classifier', vote_clf2), ('LinearSVC', vote_clf3), ('RandomForestClassifier', vote_clf4), ('XGBoost', vote_clf5)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train, y_train)\n",
    "\n",
    "classifiers = [vote_clf1, vote_clf2, vote_clf3, vote_clf4, vote_clf5, eclf1]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost', 'VotingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "predicted = eclf1.predict(X_test)\n",
    "eclf1.score(X_test, y_test)\n",
    "\n",
    "pred_results = []\n",
    "for i in predicted:\n",
    "    if i == 1:\n",
    "        pred_results.append('Consumption')\n",
    "    else:\n",
    "        pred_results.append('Other')\n",
    "\n",
    "        \n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Voting Classifier (CountVect) Clasification Report\")\n",
    "print(classification_report(class_test.tolist(),pred_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Data (tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "tfidf = TfidfVectorizer(max_df= 0.5, min_df =3, ngram_range = (1,3), norm = None)\n",
    "X_train = tfidf.fit_transform(text_train['processed_string'])\n",
    "X_train = X_train.toarray()\n",
    "y_train = class_train.replace(to_replace = \"Consumption\", value = 1)\n",
    "y_train = y_train.replace(to_replace = \"Other\", value = 0)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = tfidf.transform(text_test['processed_string'])\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "y_test = class_test.replace(to_replace = \"Consumption\", value = 1)\n",
    "y_test  = y_test.replace(to_replace = \"Other\", value = 0)\n",
    "y_test  = y_test.values\n",
    "\n",
    "X_full = tfidf.transform(full['processed_string'])\n",
    "X_full = X_full.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (tfidfVectorizer)\n",
    "## Using Logistic Regression as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.07) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.05) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.05) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.80 (+/- 0.05) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.04) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.90 (+/- 0.06) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (TF-IDF) Clasification Report (Logistic Regression as meta classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Consumption       0.91      0.95      0.93        22\n",
      "       Other       0.97      0.95      0.96        39\n",
      "\n",
      "    accuracy                           0.95        61\n",
      "   macro avg       0.94      0.95      0.95        61\n",
      "weighted avg       0.95      0.95      0.95        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.5)\n",
    "stack_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "stack_clf5 = XGBClassifier(max_depth = 6, seed = 2, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "meta_clf = LogisticRegression(C=0.5, penalty = 'l2')\n",
    "sclf_tfidf = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier', 'XGBoost', 'StackingClassifier']\n",
    "\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_tfidf = sclf_tfidf.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_tfidf.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Consumption')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "        \n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (TF-IDF) Clasification Report (Logistic Regression as meta classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier (tfidfVectorizer)\n",
    "## Using XGBoost n as meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.07) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.05) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.05) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.80 (+/- 0.05) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.04) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.05) [StackingClassifier]\n",
      "\n",
      "\n",
      "Stacked Classifier (TF-IDF) Clasification Report (XGBoost as Meta Classifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Consumption       0.91      0.91      0.91        22\n",
      "       Other       0.95      0.95      0.95        39\n",
      "\n",
      "    accuracy                           0.93        61\n",
      "   macro avg       0.93      0.93      0.93        61\n",
      "weighted avg       0.93      0.93      0.93        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ThematicTextClassify.EnsembleClassifiers import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stack_clf1 = MultinomialNB(alpha=0.5)\n",
    "stack_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "stack_clf3 = SVC(kernel='linear', C= 0.05, probability=True)\n",
    "stack_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "stack_clf5 = XGBClassifier(max_depth = 6, seed = 2, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "meta_clf = XGBClassifier(random_state=1995, seed=1, colsample_bytree=0.3, subsample=0.3)\n",
    "\n",
    "sclf_XGB_tfidf = StackingCVClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5],\n",
    "                          use_probas=True,\n",
    "                          meta_classifier=meta_clf)\n",
    "classifiers = [stack_clf1, stack_clf2, stack_clf3, stack_clf4, stack_clf5, sclf_XGB_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'StackingClassifier']\n",
    "\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "\n",
    "# Predicting new data \n",
    "sclf_XGB_tfidf = sclf_XGB_tfidf.fit(X_train,y_train)\n",
    "prediction_results_int = sclf_XGB_tfidf.predict(X_test)\n",
    "prediction_results = []\n",
    "\n",
    "for i in prediction_results_int:\n",
    "    if i == 1:\n",
    "        prediction_results.append('Consumption')\n",
    "    else:\n",
    "        prediction_results.append('Other')\n",
    "        \n",
    "# Print Classification report    \n",
    "print(\"\\n\")\n",
    "print(\"Stacked Classifier (TF-IDF) Clasification Report (XGBoost as Meta Classifier)\")\n",
    "print(classification_report(class_test.tolist(), prediction_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier (tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "5-fold cross validated Accuracy: 0.86 (+/- 0.07) [Multinomial Naive Bayes]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.05) [Logistic Regression]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.03) [Linear SVC]\n",
      "5-fold cross validated Accuracy: 0.80 (+/- 0.05) [Random Forest Classifier]\n",
      "5-fold cross validated Accuracy: 0.88 (+/- 0.04) [XGBoost]\n",
      "5-fold cross validated Accuracy: 0.89 (+/- 0.05) [VotingClassifier]\n",
      "\n",
      "\n",
      "Voting Classifier (TF-IDF) Clasification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Consumption       0.90      0.86      0.88        22\n",
      "       Other       0.93      0.95      0.94        39\n",
      "\n",
      "    accuracy                           0.92        61\n",
      "   macro avg       0.91      0.91      0.91        61\n",
      "weighted avg       0.92      0.92      0.92        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vote_clf1 = MultinomialNB(alpha=0.5)\n",
    "vote_clf2 = LogisticRegression(C=1.0, penalty = 'l2')\n",
    "vote_clf3 = LinearSVC(C=0.05, max_iter = 3000)\n",
    "vote_clf4 = RandomForestClassifier(max_depth =4, n_estimators = 200, random_state = 3)\n",
    "vote_clf5 = XGBClassifier(max_depth = 6, seed = 2, random_state=1995,colsample_bytree=0.3, subsample=0.7)\n",
    "\n",
    "\n",
    "eclf1_tfidf = VotingClassifier(estimators=[('Multinomial Naive Bayes', vote_clf1), ('Logistic Regression Classifier', vote_clf2), ('LinearSVC', vote_clf3), ('RandomForestClassifier', vote_clf4), ('XGBoost',vote_clf5)], voting='hard')\n",
    "eclf1_tfidf = eclf1_tfidf.fit(X_train, y_train)\n",
    "\n",
    "classifiers = [vote_clf1, vote_clf2, vote_clf3, vote_clf4, vote_clf5, eclf1_tfidf]\n",
    "classifier_names = ['Multinomial Naive Bayes', 'Logistic Regression', 'Linear SVC', 'Random Forest Classifier','XGBoost', 'VotingClassifier']\n",
    "\n",
    "# Acuracy function (5-fold Cross validated)\n",
    "scoring(classifiers, classifier_names, X_train, y_train)\n",
    "    \n",
    "# Predicting new data \n",
    "predicted = eclf1_tfidf.predict(X_test)\n",
    "eclf1_tfidf.score(X_test, y_test)\n",
    "\n",
    "pred_results = []\n",
    "for i in predicted:\n",
    "    if i == 1:\n",
    "        pred_results.append('Consumption')\n",
    "    else:\n",
    "        pred_results.append('Other')\n",
    "\n",
    "# Print Classification report\n",
    "print(\"\\n\")\n",
    "print(\"Voting Classifier (TF-IDF) Clasification Report\")\n",
    "print(classification_report(class_test.tolist(),pred_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending the New Categories on to the Data set\n",
    "* We pick the one model that has the highest precision/f1-score to be our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Other          1545\n",
       "Consumption     152\n",
       "Name: BestModelClassification, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full['BestModelClassification'] = sclf_log.predict(X_full)\n",
    "full['BestModelClassification'] = full['BestModelClassification'] .replace(to_replace = 1, value = \"Consumption\")\n",
    "full['BestModelClassification'] = full['BestModelClassification'] .replace(to_replace = 0, value = \"Other\")\n",
    "print(len(full[full['BestModelClassification'] == 'Consumption']))\n",
    "full['BestModelClassification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full[full['BestModelClassification'] == 'Consumption']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"BestConsumption.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
