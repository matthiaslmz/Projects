{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from utils import labelize_reviews, get_learned_vectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector (Doc2Vec)\n",
    "\n",
    "In this notebook, we'll explore the [Paragraph Vector](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) a.k.a Dov2Vec algorithm on ~3 million Yelp reviews. Doc2Vec is an extension to word2vec for learning document embeddings and basically acts as  if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. Gensim’s Doc2Vec class implements this algorithm.\n",
    "\n",
    "To recap, Word2Vec is a model from 2013 that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings\n",
    "\n",
    "There are two approaches within `doc2vec:` `dbow` and `dmpv`. \n",
    "\n",
    "`dbow (Paragraph Vector - Distributed Bag of Words)` works in the same way as `skip-gram` in word2vec ,except that the input is replaced by a special token representing the document (i.e. $v_{wI}$ is a vector representing the document). In this architecture, the order of words in the document is ignored; hence the name distributed bag of words. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "`dmpv (Paragraph Vector - Distributed Memory)` works in a similar way to `cbow` in word2vec. For the input, dmpv introduces an additional document token in addition to multiple target words. Unlike cbow, however, these vectors are not summed but concatenated (i.e. $v_{wI}$ is a concatenated vector containing the document token and several target words). The objective is again to predict a context word given the concatenated document and word vectors. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document's doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.) There are 2 DM models, specifically: \n",
    "*  one which averages context vectors (dm_mean)\n",
    "*  one which concatenates them (dm_concat, resulting in a much larger, slower, more data-hungry model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             reviews  target\n0  the rooms are big but the hotel is not good as...       0\n1  second time with ocp saturday night pm not bus...       0\n2  food is still great since they remodeled but t...       0\n3  dirty location and very high prices but they d...       0\n4  so first the off stood outside for mins to try...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviews</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the rooms are big but the hotel is not good as...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>second time with ocp saturday night pm not bus...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>food is still great since they remodeled but t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dirty location and very high prices but they d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>so first the off stood outside for mins to try...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('allcat_clean_reviews.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 3085663 entries, 0 to 3086007\nData columns (total 2 columns):\n #   Column   Dtype \n---  ------   ----- \n 0   reviews  object\n 1   target   int64 \ndtypes: int64(1), object(1)\nmemory usage: 70.6+ MB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1000\n",
    "\n",
    "x = df.reviews\n",
    "y = df.target\n",
    "\n",
    "#defining our training, validation and test set\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x, y, test_size=.06, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The Training set has 2900523 reviews with 50.00% negative, 50.00% positive reviews\nThe Validation set has 92570 entries with 50.06% negative, 49.94% positive reviews\nThe test set has a total of 92570 reviews with 49.94% negative, 50.06% positive reviews\n"
    }
   ],
   "source": [
    "\n",
    "print('The Training set has {0} reviews with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1))*100))\n",
    "\n",
    "print('The Validation set has {0} entries with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1))*100))\n",
    "\n",
    "print('The test set has a total of {0} reviews with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we label each review with a unique ID using Gensim's `TaggedDocument()` function. Then, we'll concatenate the training and validation and test sets for word representation. For training, I have decided to use the whole data set. The rationale behind this is that the Doc2Vec training is completely unsupervised (unlabelled) and thus there is no need to hold out any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nIndex: 0 entries\nEmpty DataFrame"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 1min 23s\n"
    }
   ],
   "source": [
    "%%time\n",
    "from utils import labelize_reviews\n",
    "full = pd.concat([x_train,x_validation,x_test])\n",
    "full_tagged = list(labelize_reviews(full,'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 13min 36s\n"
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "\n",
    "init_kwargs = dict(\n",
    "    vector_size=150, epochs=10, min_count=2,\n",
    "    sample=0, workers=cores, negative=5, hs=0,\n",
    "    alpha=0.05, min_alpha=0.0001, window=5\n",
    ")\n",
    "#The learning rate, alpha decreases linearly per epoch from the initial rate to the minimum rate. I will use alpha = 0.0025 and min_alpha = 0.0001 as implemented by Le and Mikolov\n",
    "#plain DBOW\n",
    "model_dbow = Doc2Vec(dm=0, **init_kwargs)\n",
    "\n",
    "model_dbow.build_vocab(full_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 15min 5s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow.build_vocab(full_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 1h 29min 23s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow.train(full_tagged, total_examples=len(full_tagged), epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save(\"dbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "~/Desktop/DATA/ghdata/Yelp/model\\dbow\\dbow.model\n"
    }
   ],
   "source": [
    "import os\n",
    "dbow_path = os.path.join(\"~\\Desktop\\DATA\\ghdata\\Yelp\\model\",'dbow','dbow.model')\n",
    "print(dbow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(\"dbow/dbow.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 26min 13s\n"
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "\n",
    "dmm_kwargs = dict(\n",
    "    vector_size=200, epochs=10, min_count=2,\n",
    "    sample=0, workers=cores, negative=5, hs=0,\n",
    "    alpha=0.05, min_alpha=0.0001, window=5\n",
    ")\n",
    "\n",
    "dmc_kwargs = dict(\n",
    "    vector_size=200, epochs=10, min_count=2,\n",
    "    sample=0, workers=cores, negative=5, hs=0,\n",
    "    alpha=0.05, min_alpha=0.0001, window=3\n",
    ")\n",
    "#Distributed Memory (mean)\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, **dmm_kwargs)\n",
    "    \n",
    "# Distributed Memory(Concatenation)\n",
    "model_dmc = Doc2Vec(dm=1, dm_concat=1, **dmc_kwargs)\n",
    "\n",
    "model_dmm.build_vocab(full_tagged)\n",
    "model_dmc.build_vocab(full_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2h 30min 7s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dmm.train(full_tagged, total_examples=len(full_tagged), epochs=model_dmm.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm.save(\"dmm/dmm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 1h 41min 21s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dmc.train(full_tagged, total_examples=len(full_tagged), epochs=model_dmc.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmc.save(\"dmc/dmc.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with DBOW, DMM (Mean), DMC (Concatenation)\n",
    "\n",
    "Given a document, our Doc2Vec models output a vector representation of the document. How useful is a particular model? In case of sentiment classification, we want the ouput vector to reflect the sentiment in the input document. So, in vector space, positive documents should be distant from negative documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learned_vectors(model,corpus):\n",
    "    \"\"\"\n",
    "    A function that extracts document vectors from a TRAINED Doc2Vec model\n",
    "    \n",
    "    model: Trained Doc2Vec model \n",
    "    \"\"\"\n",
    "    vecs = [model.docvecs['all_'+str(ind)] for ind, doc in corpus.iteritems()]\n",
    "    \n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow = Doc2Vec.load(\"dbow.model\")\n",
    "model_dm_avg = Doc2Vec.load(\"dm_avg.model\")\n",
    "model_dm_cat = Doc2Vec.load(\"dm_cat.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBOW Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.28415165036804385 \nValidation Accuracy: 0.881981203413633\nWall time: 1min 19s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_dbow = get_learned_vectors(model_dbow, x_train)\n",
    "validation_vecs_dbow = get_learned_vectors(model_dbow, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow)\n",
    "\n",
    "logloss_dbow = log_loss(y_validation, y_pred)\n",
    "acc= clf.score(validation_vecs_dbow, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dbow, \"\\nValidation Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.2901750885047062 \nValidation Accuracy: 0.8821756508588096\nWall time: 4min 29s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_dmm = get_learned_vectors(model_dmm, x_train)\n",
    "validation_vecs_dmm = get_learned_vectors(model_dmm, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dmm)\n",
    "\n",
    "logloss_dmm = log_loss(y_validation, y_pred)\n",
    "acc_dmm = clf.score(validation_vecs_dmm, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dmm, \"\\nValidation Accuracy:\", acc_dmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMC Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.6930191077491353 \nValidation Accuracy: 0.5046343307767095\nWall time: 2min 28s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_dmc = get_learned_vectors(model_dmc, x_train)\n",
    "validation_vecs_dmc = get_learned_vectors(model_dmc, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmc, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dmc)\n",
    "\n",
    "logloss_dmc = log_loss(y_validation, y_pred)\n",
    "acc_dmc = clf.score(validation_vecs_dmc, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dmc, \"\\nValidation Accuracy:\", acc_dmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le and Mikolov notes that combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. We will follow, pairing the models together for evaluation. Here, we concatenate the paragraph vectors obtained from each model with the help of a thin wrapper class included in a gensim test module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "dbow_dmm = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "dbow_dmc = ConcatenatedDoc2Vec([model_dbow, model_dmc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW + DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.21934084701063036 \nValidation Accuracy: 0.9125418602138922\n"
    }
   ],
   "source": [
    "train_vecs_dbow_dmm = get_learned_vectors(dbow_dmm,x_train)\n",
    "validation_vecs_dbow_dmm = get_learned_vectors(dbow_dmm, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmm,y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmm)\n",
    "logloss_dbowdmm = log_loss(y_validation,y_pred)\n",
    "acc_dbowdmm = clf.score(validation_vecs_dbow_dmm, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dbowdmm, \"\\nValidation Accuracy:\", acc_dbowdmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW + DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.28430528204900746 \nValidation Accuracy: 0.881916387598574\n"
    }
   ],
   "source": [
    "train_vecs_dbow_dmc = get_learned_vectors(dbow_dmc,x_train)\n",
    "validation_vecs_dbow_dmc = get_learned_vectors(dbow_dmc, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmc,y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmc)\n",
    "logloss_dbowdmc = log_loss(y_validation,y_pred)\n",
    "acc_dbowdmc = clf.score(validation_vecs_dbow_dmc, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dbowdmc, \"\\nValidation Accuracy:\", acc_dbowdmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pleasant', 0.7412006258964539),\n",
       " ('fantastic', 0.718704342842102),\n",
       " ('phenomenal', 0.6945322751998901),\n",
       " ('awesome', 0.6865039467811584),\n",
       " ('superb', 0.6821604371070862),\n",
       " ('sloppy', 0.678415060043335),\n",
       " ('incredible', 0.6747259497642517),\n",
       " ('speedy', 0.6609603762626648),\n",
       " ('awful', 0.6602469682693481),\n",
       " ('amazing', 0.6575909852981567)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dm_avg.wv.most_similar(\"wonderful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tasty', 0.7437097430229187),\n",
       " ('decent', 0.6630314588546753),\n",
       " ('cool', 0.6535404920578003),\n",
       " ('solid', 0.570975124835968),\n",
       " ('inconsistent', 0.5689284801483154),\n",
       " ('bad', 0.549915075302124),\n",
       " ('cheap', 0.5468442440032959),\n",
       " ('yummy', 0.5432661771774292),\n",
       " ('scary', 0.5387668013572693),\n",
       " ('great', 0.5234598517417908)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dm_avg.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brunch', 0.7673641443252563),\n",
       " ('lunch', 0.7425373792648315),\n",
       " ('supper', 0.6696999669075012),\n",
       " ('starters', 0.6494625210762024),\n",
       " ('searching', 0.6484907865524292),\n",
       " ('meeting', 0.6315172910690308),\n",
       " ('breakfast', 0.6313809156417847),\n",
       " ('graduation', 0.6271716356277466),\n",
       " ('dessert', 0.6249339580535889),\n",
       " ('annual', 0.6146324872970581)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dm_avg.wv.most_similar(\"dinner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Modeling\n",
    "Another thing that can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently-used phrases and sticks them together.\n",
    "\n",
    "$$\\frac{{count(A B)}-{count_{min}}} {{count(A)} \\times {count(B)}} \\times \\text{N} \\gt \\text{threshhold} $$\n",
    "\n",
    "where:\n",
    "\n",
    "count(A) is the number of times token A appears in the corpus <br/>\n",
    "count(B) is the number of times token B appears in the corpus <br/>\n",
    "count(A B) is the number of times the tokens A B appear in the corpus in order <br/>\n",
    "N is the total size of the corpus vocabulary <br/>\n",
    "count_{min} is a user-defined parameter to ensure that accepted phrases occur a minimum number of times <br/>\n",
    "threshold is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase (default threshold used in Gensim's Phrases function is 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train = [r.split() for r in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phrases = Phrases(tok_train)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_reviews_bg(reviews,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, j in zip(reviews.index,reviews):\n",
    "        result.append(TaggedDocument(bigram[j.split()],[prefix + '_%s' % i ]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_tagged_bg= pd.concat([x_train,x_validation,x_test])\n",
    "full_taggedw2v_bg = labelize_reviews_bg(full_tagged_bg, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(full_taggedw2v_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085663"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_taggedw2v_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBOW Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2578879.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#plain DBOW bigram\n",
    "\n",
    "model_dbow_bg = Doc2Vec(dm=0, vector_size=150, negative=5, min_count=2, workers=cores, alpha=0.05,sample=0)\n",
    "model_dbow_bg.build_vocab([x for x in tqdm(full_taggedw2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2898672.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3141000.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3176412.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3131359.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3038423.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3143782.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2777146.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3027322.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3114985.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3017679.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2521502.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3176449.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3172518.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3181764.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3169988.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3018469.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2970609.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3156972.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2884894.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3192871.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3163370.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2358063.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3168702.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3164072.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2785559.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3176472.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3219329.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3228587.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2952181.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3235735.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 12min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow_bg.train([x for x in tqdm(full_taggedw2v_bg)], total_examples=len(full_taggedw2v_bg), epochs=1)\n",
    "    model_dbow_bg.alpha -= 0.002\n",
    "    model_dbow_bg.min_alpha = model_dbow_bg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow_bg.save(\"dbow_bg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2091432079255557"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_dbow = Doc2Vec.load(\"dbow_bg.model\")\n",
    "train_vecs_dbow_bg = get_vectors(model_dbow_bg,x_train)\n",
    "validation_vecs_dbow_bg = get_vectors(model_dbow_bg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_bg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_bg)\n",
    "logloss_dbow_bg = log_loss(y_validation,y_pred)\n",
    "logloss_dbow_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3085663/3085663 [00:09<00:00, 338270.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 1849920.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "#Distributed Memory (mean) BIGRAM\n",
    "model_dmm_bg = Doc2Vec(dm=1, dm_mean=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "    \n",
    "#Distributed Memory (Concatenation) BIGRAM\n",
    "model_dmc_bg = Doc2Vec(dm=1,dm_concat=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "\n",
    "#model_dm_cat1 = Doc2Vec(dm=1,dm_concat=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "\n",
    "model_dmm_bg.build_vocab([x for x in tqdm(full_taggedw2v_bg)])\n",
    "model_dmc_bg.build_vocab([x for x in tqdm(full_taggedw2v_bg)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMM BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2733320.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2835982.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2795023.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2823996.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2614992.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2815304.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2835841.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2918933.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2565405.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2886649.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2857066.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2825692.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2846470.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2867717.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 3085663/3085663 [00:03<00:00, 927170.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3077804.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2745264.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3075269.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2917835.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2773399.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2809705.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2936052.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3043501.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2975296.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3174155.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2890422.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3118881.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3155346.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 3027347.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2885363.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 26min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dmm_bg.train([x for x in tqdm(full_taggedw2v_bg)], total_examples=len(full_taggedw2v_bg), epochs=1)\n",
    "    model_dmm_bg.alpha -= 0.002\n",
    "    model_dmm_bg.min_alpha = model_dmm_bg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm_bg.save(\"dbow_dmm_bg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow_bg = Doc2Vec.load(\"dbow_bg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm_bg = Doc2Vec.load(\"dbow_dmm_bg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625711287309628"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dmm_bg = get_vectors(model_dmm_bg,x_train)\n",
    "validation_vecs_dmm_bg = get_vectors(model_dmm_bg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmm_bg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dmm_bg)\n",
    "logloss_dmm_bg = log_loss(y_validation,y_pred)\n",
    "logloss_dmm_bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMC BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2796892.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "\n",
    "#Distributed Memory (Concatenation) BIGRAM\n",
    "model_dmc_bg = Doc2Vec(dm=1,dm_concat=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "model_dmc_bg.build_vocab([x for x in tqdm(full_taggedw2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2551664.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2191942.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2244352.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2126928.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 1938741.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2460026.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2089500.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2709848.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2502123.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2338797.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2531248.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2210295.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2784417.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2635842.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2780767.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2494325.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2719342.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2992333.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2704029.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2392153.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2670027.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2822285.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2734805.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2711888.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2748822.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2668023.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2771038.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2733927.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2784343.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2758447.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4h 33min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dmc_bg.train([x for x in tqdm(full_taggedw2v_bg)], total_examples=len(full_taggedw2v_bg), epochs=1)\n",
    "    model_dmc_bg.alpha -= 0.002\n",
    "    model_dmc_bg.min_alpha = model_dmc_bg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmc_bg.save(\"dmc_bg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.693094161261186"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dmc_bg = get_vectors(model_dmc_bg,x_train)\n",
    "validation_vecs_dmc_bg = get_vectors(model_dmc_bg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmc_bg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dmc_bg)\n",
    "logloss_dmc_bg = log_loss(y_validation,y_pred)\n",
    "logloss_dmc_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmc_bg = Doc2Vec.load(\"dmc_bg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "dbow_dmm_bg = ConcatenatedDoc2Vec([model_dbow_bg, model_dmm_bg])\n",
    "dbow_dmc_bg = ConcatenatedDoc2Vec([model_dbow_bg, model_dmc_bg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2069170050900326"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dmm_bg = get_vectors(dbow_dmm_bg,x_train)\n",
    "validation_vecs_dbow_dmm_bg = get_vectors(dbow_dmm_bg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmm_bg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmm_bg)\n",
    "logloss_dbowdmm_bg = log_loss(y_validation,y_pred)\n",
    "logloss_dbowdmm_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2091432070267747"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dmc_bg = get_vectors(dbow_dmc_bg,x_train)\n",
    "validation_vecs_dbow_dmc_bg = get_vectors(dbow_dmc_bg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmc_bg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmc_bg)\n",
    "logloss_dbowdmc_bg = log_loss(y_validation,y_pred)\n",
    "logloss_dbowdmc_bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram\n",
    "\n",
    "If we run the same phrase detection again on bigram detected corpus, now it will detect trigram phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phrases_tri = Phrases(bigram[tok_train])\n",
    "trigram = Phraser(phrases_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_reviews_tg(reviews,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, j in zip(reviews.index,reviews):\n",
    "        result.append(TaggedDocument(trigram[bigram[j.split()]],[prefix + '_%s' % i ]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 13min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_tagged_tg= pd.concat([x_train,x_validation,x_test])\n",
    "full_taggedw2v_tg = labelize_reviews_tg(full_tagged_tg, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(full_taggedw2v_tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2338471.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#plain DBOW trigram\n",
    "\n",
    "model_dbow_tg = Doc2Vec(dm=0, vector_size=150, negative=5, min_count=2, workers=cores, alpha=0.05,sample=0)\n",
    "model_dbow_tg.build_vocab([x for x in tqdm(full_taggedw2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2237168.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2906597.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2897456.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2958828.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2945361.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2973237.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2408721.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 3085663/3085663 [00:03<00:00, 779209.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 3085663/3085663 [00:03<00:00, 835766.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2816393.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2819638.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2843645.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2929288.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2880618.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2932496.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2882022.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2781869.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2933892.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2871490.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2866972.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2858920.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2881858.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2935374.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2940326.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2920854.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2935412.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2924288.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2832592.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2929053.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2932641.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 39min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow_tg.train([x for x in tqdm(full_taggedw2v_tg)], total_examples=len(full_taggedw2v_tg), epochs=1)\n",
    "    model_dbow_tg.alpha -= 0.002\n",
    "    model_dbow_tg.min_alpha = model_dbow_tg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow_tg.save(\"dbow_tg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow_tg = Doc2Vec.load(\"dbow_tg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20806212875146537"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_dbow = Doc2Vec.load(\"dbow_bg.model\")\n",
    "train_vecs_dbow_tg = get_vectors(model_dbow_tg,x_train)\n",
    "validation_vecs_dbow_tg = get_vectors(model_dbow_tg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_tg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_tg)\n",
    "logloss_dbow_tg = log_loss(y_validation,y_pred)\n",
    "logloss_dbow_tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2673284.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2413416.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "#Distributed Memory (mean) BIGRAM\n",
    "model_dmm_tg = Doc2Vec(dm=1, dm_mean=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "    \n",
    "#Distributed Memory (Concatenation) BIGRAM\n",
    "model_dmc_tg = Doc2Vec(dm=1,dm_concat=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "\n",
    "#model_dm_cat1 = Doc2Vec(dm=1,dm_concat=1, vector_size=150, window=8, negative=5, hs=0, min_count=2, workers=cores, alpha=0.05, min_alpha=0.0001,epochs=30, sample=0)\n",
    "\n",
    "model_dmm_tg.build_vocab([x for x in tqdm(full_taggedw2v_tg)])\n",
    "model_dmc_tg.build_vocab([x for x in tqdm(full_taggedw2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2678458.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2565954.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2664673.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3095948.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2407243.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3092953.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2981727.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:00<00:00, 3143782.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2327950.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2787355.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2997942.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2908776.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2968360.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2969437.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2913296.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2956947.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2970223.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2421336.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2935774.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2711204.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2745089.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2673720.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2152968.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2168084.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2413569.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2329860.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2801345.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2104620.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2524906.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 3085663/3085663 [00:01<00:00, 2646154.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dmm_tg.train([x for x in tqdm(full_taggedw2v_tg)], total_examples=len(full_taggedw2v_tg), epochs=1)\n",
    "    model_dmm_tg.alpha -= 0.002\n",
    "    model_dmm_tg.min_alpha = model_dmm_tg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm_tg.save(\"dmm_tg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm_tg = Doc2Vec.load(\"dmm_tg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3275229428405004"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dmm_tg = get_vectors(model_dmm_tg,x_train)\n",
    "validation_vecs_dmm_tg = get_vectors(model_dmm_tg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmm_tg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dmm_tg)\n",
    "logloss_dmm_tg = log_loss(y_validation,y_pred)\n",
    "logloss_dmm_tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "dbow_dmm_tg = ConcatenatedDoc2Vec([model_dbow_tg, model_dmm_tg])\n",
    "#dbow_dmc_tg = ConcatenatedDoc2Vec([model_dbow_tg, model_dmc_tg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19275270351977944"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dmm_tg = get_vectors(dbow_dmm_tg,x_train)\n",
    "validation_vecs_dbow_dmm_tg = get_vectors(dbow_dmm_tg, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmm_tg,y_train)\n",
    "#clf.score(validation_vecs_dbow,y_validation)\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmm_tg)\n",
    "logloss_dbowdmm_tg = log_loss(y_validation,y_pred)\n",
    "logloss_dbowdmm_tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}