{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from utils import labelize_reviews, get_learned_vectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector (Doc2Vec)\n",
    "\n",
    "In this notebook, we'll explore the [Paragraph Vector](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) a.k.a Dov2Vec algorithm on ~3 million Yelp reviews. Doc2Vec is an extension to word2vec for learning document embeddings and basically acts as  if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. Gensimâ€™s Doc2Vec class implements this algorithm.\n",
    "\n",
    "To recap, Word2Vec is a model from 2013 that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings\n",
    "\n",
    "There are two approaches within `doc2vec:` `dbow` and `dmpv`. \n",
    "\n",
    "`dbow (Paragraph Vector - Distributed Bag of Words)` works in the same way as `skip-gram` in word2vec ,except that the input is replaced by a special token representing the document (i.e. $v_{wI}$ is a vector representing the document). In this architecture, the order of words in the document is ignored; hence the name distributed bag of words. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "`dmpv (Paragraph Vector - Distributed Memory)` works in a similar way to `cbow` in word2vec. For the input, dmpv introduces an additional document token in addition to multiple target words. Unlike cbow, however, these vectors are not summed but concatenated (i.e. $v_{wI}$ is a concatenated vector containing the document token and several target words). The objective is again to predict a context word given the concatenated document and word vectors. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document's doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.) There are 2 DM models, specifically: \n",
    "*  one which averages context vectors (dm_mean)\n",
    "*  one which concatenates them (dm_concat, resulting in a much larger, slower, more data-hungry model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             reviews  target\n0  the rooms are big but the hotel is not good as...       0\n1  second time with ocp saturday night pm not bus...       0\n2  food is still great since they remodeled but t...       0\n3  dirty location and very high prices but they d...       0\n4  so first the off stood outside for mins to try...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviews</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the rooms are big but the hotel is not good as...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>second time with ocp saturday night pm not bus...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>food is still great since they remodeled but t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dirty location and very high prices but they d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>so first the off stood outside for mins to try...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('allcat_clean_reviews.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 3085663 entries, 0 to 3086007\nData columns (total 2 columns):\n #   Column   Dtype \n---  ------   ----- \n 0   reviews  object\n 1   target   int64 \ndtypes: int64(1), object(1)\nmemory usage: 70.6+ MB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1000\n",
    "\n",
    "x = df.reviews\n",
    "y = df.target\n",
    "\n",
    "#defining our training, validation and test set\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x, y, test_size=.06, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The Training set has 2900523 reviews with 50.00% negative, 50.00% positive reviews\nThe Validation set has 92570 entries with 50.06% negative, 49.94% positive reviews\nThe test set has a total of 92570 reviews with 49.94% negative, 50.06% positive reviews\n"
    }
   ],
   "source": [
    "\n",
    "print('The Training set has {0} reviews with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1))*100))\n",
    "\n",
    "print('The Validation set has {0} entries with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1))*100))\n",
    "\n",
    "print('The test set has a total of {0} reviews with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we label each review with a unique ID using Gensim's `TaggedDocument()` function. Then, we'll concatenate the training and validation and test sets for word representation. For training, I have decided to use the whole data set. The rationale behind this is that the Doc2Vec training is completely unsupervised (unlabelled) and thus there is no need to hold out any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nIndex: 0 entries\nEmpty DataFrame"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2min 29s\n"
    }
   ],
   "source": [
    "%%time\n",
    "from utils import labelize_reviews\n",
    "full = pd.concat([x_train,x_validation,x_test])\n",
    "full_tagged = list(labelize_reviews(full,'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 13min 36s\n"
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "\n",
    "init_kwargs = dict(\n",
    "    vector_size=150, epochs=10, min_count=2,\n",
    "    sample=0, workers=cores, negative=5, hs=0,\n",
    "    alpha=0.05, min_alpha=0.0001, window=5\n",
    ")\n",
    "#The learning rate, alpha decreases linearly per epoch from the initial rate to the minimum rate. I will use alpha = 0.0025 and min_alpha = 0.0001 as implemented by Le and Mikolov\n",
    "#plain DBOW\n",
    "model_dbow = Doc2Vec(dm=0, **init_kwargs)\n",
    "\n",
    "model_dbow.build_vocab(full_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 1h 29min 23s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow.train(full_tagged, total_examples=len(full_tagged), epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save(\"dbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 26min 13s\n"
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count() #12\n",
    "\n",
    "dmm_kwargs = dict(\n",
    "    vector_size=200, epochs=10, min_count=2,\n",
    "    sample=0, workers=cores, negative=5, hs=0,\n",
    "    alpha=0.05, min_alpha=0.0001, window=5\n",
    ")\n",
    "\n",
    "dmc_kwargs = dict(\n",
    "    vector_size=200, epochs=10, min_count=2,\n",
    "    sample=0, workers=cores, negative=5, hs=0,\n",
    "    alpha=0.05, min_alpha=0.0001, window=3\n",
    ")\n",
    "#Distributed Memory (mean)\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, **dmm_kwargs)\n",
    "    \n",
    "# Distributed Memory(Concatenation)\n",
    "model_dmc = Doc2Vec(dm=1, dm_concat=1, **dmc_kwargs)\n",
    "\n",
    "model_dmm.build_vocab(full_tagged)\n",
    "model_dmc.build_vocab(full_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2h 30min 7s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dmm.train(full_tagged, total_examples=len(full_tagged), epochs=model_dmm.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm.save(\"dmm/dmm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 1h 41min 21s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dmc.train(full_tagged, total_examples=len(full_tagged), epochs=model_dmc.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmc.save(\"dmc/dmc.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with DBOW, DMM (Mean), DMC (Concatenation)\n",
    "\n",
    "Given a document, our Doc2Vec models output a vector representation of the document. How useful is a particular model? In case of sentiment classification, we want the ouput vector to reflect the sentiment in the input document. So, in vector space, positive documents should be distant from negative documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 1min 22s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow = Doc2Vec.load(\"dbow/dbow.model\")\n",
    "model_dmm = Doc2Vec.load(\"dmm/dmm.model\")\n",
    "model_dmc = Doc2Vec.load(\"dmc/dmc.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBOW Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.28415165036804385 \nValidation Accuracy: 0.881981203413633\nWall time: 1min 19s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_dbow = get_learned_vectors(model_dbow, x_train)\n",
    "validation_vecs_dbow = get_learned_vectors(model_dbow, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow)\n",
    "\n",
    "logloss_dbow = log_loss(y_validation, y_pred)\n",
    "acc= clf.score(validation_vecs_dbow, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dbow, \"\\nValidation Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.2901750885047062 \nValidation Accuracy: 0.8821756508588096\nWall time: 4min 29s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_dmm = get_learned_vectors(model_dmm, x_train)\n",
    "validation_vecs_dmm = get_learned_vectors(model_dmm, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dmm)\n",
    "\n",
    "logloss_dmm = log_loss(y_validation, y_pred)\n",
    "acc_dmm = clf.score(validation_vecs_dmm, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dmm, \"\\nValidation Accuracy:\", acc_dmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMC Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.6930191077491353 \nValidation Accuracy: 0.5046343307767095\nWall time: 2min 28s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_dmc = get_learned_vectors(model_dmc, x_train)\n",
    "validation_vecs_dmc = get_learned_vectors(model_dmc, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dmc, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dmc)\n",
    "\n",
    "logloss_dmc = log_loss(y_validation, y_pred)\n",
    "acc_dmc = clf.score(validation_vecs_dmc, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dmc, \"\\nValidation Accuracy:\", acc_dmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le and Mikolov notes that combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. We will follow, pairing the models together for evaluation. So, we'll concatenate the paragraph vectors obtained from each model using the `ConcatenatedDoc2Vec` function from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "dbow_dmm = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "dbow_dmc = ConcatenatedDoc2Vec([model_dbow, model_dmc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW + DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.21934084701063036 \nValidation Accuracy: 0.9125418602138922\n"
    }
   ],
   "source": [
    "train_vecs_dbow_dmm = get_learned_vectors(dbow_dmm,x_train)\n",
    "validation_vecs_dbow_dmm = get_learned_vectors(dbow_dmm, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmm,y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmm)\n",
    "logloss_dbowdmm = log_loss(y_validation,y_pred)\n",
    "acc_dbowdmm = clf.score(validation_vecs_dbow_dmm, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dbowdmm, \"\\nValidation Accuracy:\", acc_dbowdmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW + DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation Logloss: 0.28430528204900746 \nValidation Accuracy: 0.881916387598574\n"
    }
   ],
   "source": [
    "train_vecs_dbow_dmc = get_learned_vectors(dbow_dmc,x_train)\n",
    "validation_vecs_dbow_dmc = get_learned_vectors(dbow_dmc, x_validation)\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(train_vecs_dbow_dmc,y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(validation_vecs_dbow_dmc)\n",
    "logloss_dbowdmc = log_loss(y_validation,y_pred)\n",
    "acc_dbowdmc = clf.score(validation_vecs_dbow_dmc, y_validation)\n",
    "print(\"Validation Logloss:\", logloss_dbowdmc, \"\\nValidation Accuracy:\", acc_dbowdmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the result above, we see that concatenating both DMM and DBOW models has increased our accuracy by ~ 3%. The best validation accuracy I got from a single model is the DMM model at 88.22% but the results are fairly similar to the DMC model at 88.20%\n",
    "\n",
    "\n",
    "Now that we have trained several models, let's take a look at the awesomeness of a trained word embedding! An incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. For example, if we can see that the model has learnt the relationship between dinner, lunch and breakfast and see that dinner - afternoon = lunch. Since all the models we trained are on the YELP dataset that contains mostly of food reviews, let's look at some syntactic/semantic NLP word tasks with the trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('lunch', 0.48653391003608704), ('breakfast', 0.48273783922195435)]"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "model_dmm.wv.most_similar(positive=['dinner'], negative=['afternoon'])[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('fettuccine', 0.7643482685089111),\n ('gnocchi', 0.7500354647636414),\n ('risotto', 0.7493999004364014),\n ('spaghetti', 0.7333984375),\n ('linguine', 0.7279767990112305),\n ('linguini', 0.7070115804672241),\n ('penne', 0.7034958600997925),\n ('lasagna', 0.686015248298645),\n ('fettuccini', 0.6845107078552246),\n ('ravioli', 0.6838340759277344)]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "#it's interesting to see that the model is able to recognize the different type of\n",
    "model_dmm.wv.most_similar('pasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'cereal'"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# Pick the odd one out!\n",
    "model_dmm.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Result Summary for different variations of Dov2Vec on Validation set **\n",
    "\n",
    "| Models     | unigram |\n",
    "|------------|---------|\n",
    "| DBOW       |  88.20% |\n",
    "| DMM        |  88.22% |\n",
    "| DMC        |  50.46% |\n",
    "| dbow + dmc |  91.25% |\n",
    "| dbow + dmm |  88.19% |\n",
    "\n",
    "In summary, we see that the combined model of dbow+dmc performed the best and like Le and Mikolov mentioned, combining two models does indeed improve the performance. In the next notebook, `bigram_Doc2Vec.ipynb`, we will explore the concept of phrase modelling, which is essentially the detection of common phrases. More information on this can be found [here](https://radimrehurek.com/gensim/models/phrases.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}