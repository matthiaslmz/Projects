{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, log_loss\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "import os\n",
    "from path import general_path\n",
    "from utils import labelize_reviews_bg\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector (Doc2Vec)\n",
    "\n",
    "In this notebook, we'll explore the [Paragraph Vector](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) a.k.a Dov2Vec algorithm on ~3 million Yelp reviews. Doc2Vec is an extension to word2vec for learning document embeddings and basically acts as  if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. Gensimâ€™s Doc2Vec class implements this algorithm.\n",
    "\n",
    "To recap, Word2Vec is a model from 2013 that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings\n",
    "\n",
    "There are two approaches within `doc2vec:` `dbow` and `dmpv`. \n",
    "\n",
    "`dbow (Paragraph Vector - Distributed Bag of Words)` works in the same way as `skip-gram` in word2vec ,except that the input is replaced by a special token representing the document (i.e. $v_{wI}$ is a vector representing the document). In this architecture, the order of words in the document is ignored; hence the name distributed bag of words. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "`dmpv (Paragraph Vector - Distributed Memory)` works in a similar way to `cbow` in word2vec. For the input, dmpv introduces an additional document token in addition to multiple target words. Unlike cbow, however, these vectors are not summed but concatenated (i.e. $v_{wI}$ is a concatenated vector containing the document token and several target words). The objective is again to predict a context word given the concatenated document and word vectors. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document's doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.) There are 2 DM models, specifically: \n",
    "*  one which averages context vectors (dm_mean)\n",
    "*  one which concatenates them (dm_concat, resulting in a much larger, slower, more data-hungry model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             reviews  target\n0  the rooms are big but the hotel is not good as...       0\n1  second time with ocp saturday night pm not bus...       0\n2  food is still great since they remodeled but t...       0\n3  dirty location and very high prices but they d...       0\n4  so first the off stood outside for mins to try...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviews</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the rooms are big but the hotel is not good as...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>second time with ocp saturday night pm not bus...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>food is still great since they remodeled but t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dirty location and very high prices but they d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>so first the off stood outside for mins to try...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('allcat_clean_reviews.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 3085663 entries, 0 to 3086007\nData columns (total 2 columns):\n #   Column   Dtype \n---  ------   ----- \n 0   reviews  object\n 1   target   int64 \ndtypes: int64(1), object(1)\nmemory usage: 70.6+ MB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1000\n",
    "x = df.reviews\n",
    "y = df.target\n",
    "\n",
    "#defining our training, validation and test set\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x, y, test_size=.06, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The Training set has 2900523 reviews with 50.00% negative, 50.00% positive reviews\nThe Validation set has 92570 entries with 50.06% negative, 49.94% positive reviews\nThe test set has a total of 92570 reviews with 49.94% negative, 50.06% positive reviews\n"
    }
   ],
   "source": [
    "\n",
    "print('The Training set has {0} reviews with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1))*100))\n",
    "\n",
    "print('The Validation set has {0} entries with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1))*100))\n",
    "\n",
    "print('The test set has a total of {0} reviews with {1:.2f}% negative, {2:.2f}% positive reviews'.format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 118 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "from utils import labelize_reviews\n",
    "full = pd.concat([x_train,x_validation,x_test])\n",
    "full_tagged = list(labelize_reviews(full,\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Modeling\n",
    "Another thing that can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently-used phrases and sticks them together.\n",
    "\n",
    "$$\\frac{{count(A B)}-{count_{min}}} {{count(A)} \\times {count(B)}} \\times \\text{N} \\gt \\text{threshhold} $$\n",
    "\n",
    "where:\n",
    "\n",
    "count(A) is the number of times token A appears in the corpus <br/>\n",
    "count(B) is the number of times token B appears in the corpus <br/>\n",
    "count(A B) is the number of times the tokens A B appear in the corpus in order <br/>\n",
    "N is the total size of the corpus vocabulary <br/>\n",
    "count_{min} is a user-defined parameter to ensure that accepted phrases occur a minimum number of times <br/>\n",
    "threshold is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase (default threshold used in Gensim's Phrases function is 10.0)\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When the model encounters 2 tokens in the new text that identifies as a phrase, it will merge the two into a single new token. \n",
    "\n",
    "Phrase modelling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york becomes new_york). But you would also expect multi-word expressions that represent common concepts, but arne't specifically named entities (such as *happy hour*) to also become phrases in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_trained = (tokens.split() for tokens in x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 10min 55s\n"
    }
   ],
   "source": [
    "%%time\n",
    "bigram_model_path = os.path.join(general_path, 'YELP', 'model')\n",
    "phrases_bigram = Phrases(tokens_trained, min_count=1)\n",
    "\n",
    "# Turn the finished Phrases model into a \"Phraser\" object that is optimized for speed and memory use\n",
    "bigram_phrases = Phraser(phrases_bigram)\n",
    "#bigram_phrases.save(bigram_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we can see that the model has learn that ice cream is a frequently used term and concatenated them together as a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['i', 'love', 'ice_cream']"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "ex = [u'i', u'love', u'ice', u'cream']\n",
    "bigram_phrases[ex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we label each review with a unique ID using Gensim's `TaggedDocument()` function. Then, we'll concatenate the training, validation and test sets for word representation. This is because doc2vec training are completely unsupervised and thus there is no need to hold out any data as it's unlabelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "full = pd.concat([x_train,x_validation,x_test])\n",
    "full_tagged = list(labelize_reviews_bg(full,'all', bigram_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}